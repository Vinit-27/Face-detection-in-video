{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "id": "Xza33OrY9I7I",
    "outputId": "17f541ca-f801-4600-85d4-a057532c413f"
   },
   "outputs": [],
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1, fixed_image_standardization, training\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import math\n",
    "from PIL import Image\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'Faces'\n",
    "batch_size = 8\n",
    "epochs = 30\n",
    "workers = 0 if os.name == 'nt' else 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastMTCNN(object):\n",
    "    \"\"\"Fast MTCNN implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, stride, resize=1, *args, **kwargs):\n",
    "        \"\"\"Constructor for FastMTCNN class.\n",
    "        \n",
    "        Arguments:\n",
    "            stride (int): The detection stride. Faces will be detected every `stride` frames\n",
    "                and remembered for `stride-1` frames.\n",
    "        \n",
    "        Keyword arguments:\n",
    "            resize (float): Fractional frame scaling. [default: {1}]\n",
    "            *args: Arguments to pass to the MTCNN constructor. See help(MTCNN).\n",
    "            **kwargs: Keyword arguments to pass to the MTCNN constructor. See help(MTCNN).\n",
    "        \"\"\"\n",
    "        self.stride = stride\n",
    "        self.resize = resize\n",
    "        self.mtcnn = MTCNN(*args, **kwargs)\n",
    "        \n",
    "    def __call__(self, frames,save_path):\n",
    "        \"\"\"Detect faces in frames using strided MTCNN.\"\"\"\n",
    "        if self.resize != 1:\n",
    "            frames = [\n",
    "                cv2.resize(f, (int(f.shape[1] * self.resize), int(f.shape[0] * self.resize)))\n",
    "                    for f in frames\n",
    "            ]\n",
    "        else:\n",
    "            frames_resized = [np.array(frames)]\n",
    "                      \n",
    "        boxes, probs = self.mtcnn.detect(frames_resized[::self.stride])\n",
    "\n",
    "        faces = []\n",
    "        for i, frame in enumerate(frames_resized):\n",
    "            box_ind = int(i / self.stride)\n",
    "            if boxes[box_ind] is None:\n",
    "                continue\n",
    "            for j, box in enumerate(boxes[box_ind]):\n",
    "                box = [int(b) for b in box]\n",
    "                faces.append(frame[box[1]:box[3], box[0]:box[2]])\n",
    "                frame_pil = Image.fromarray(frame)\n",
    "                cropped_image = frame_pil.crop((box[0], box[1], box[2], box[3]))\n",
    "                timestamp = int(time.time() * 1000)\n",
    "                cropped_image.save(os.path.join(save_path, f\"cropped_{timestamp}.jpg\"))\n",
    "        \n",
    "        return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcnn = FastMTCNN(\n",
    "    image_size=160, margin=0, min_face_size=30,\n",
    "    thresholds=[0.4, 0.4, 0.4], factor=0.709, post_process=True,\n",
    "    device=device,stride=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(data_dir, transform=transforms.Resize((512, 512)))\n",
    "dataset.samples = [\n",
    "    (p, p.replace(data_dir, data_dir + '_cropped'))\n",
    "        for p, _ in dataset.samples\n",
    "]\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    num_workers=workers,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=training.collate_pil\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 58 of 58"
     ]
    }
   ],
   "source": [
    "for i, (x, y) in enumerate(loader):\n",
    "    # print(len(x))\n",
    "    for img,path in zip(x,y):\n",
    "        save_dir = os.path.dirname(path)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        mtcnn(img,save_path=save_dir)\n",
    "    print('\\rBatch {} of {}'.format(i + 1, len(loader)), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove mtcnn to reduce GPU memory usage\n",
    "del mtcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.RandomResizedCrop(size=(128, 128), scale=(0.8, 1.0)),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.GaussianBlur(kernel_size=3),\n",
    "    transforms.Resize(size=(160, 160)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transform\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageFolder(root='Faces_cropped', transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images saved successfully.\n"
     ]
    }
   ],
   "source": [
    "save_dir = 'Faces_cropped'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save augmented images\n",
    "for i, (images, labels) in enumerate(dataloader):\n",
    "    for j, (image, label) in enumerate(zip(images, labels)):\n",
    "        if label==0:\n",
    "            label_dir = os.path.join(save_dir, f'Kush')\n",
    "        elif label==1:\n",
    "            label_dir = os.path.join(save_dir, f'Namit')\n",
    "        os.makedirs(label_dir, exist_ok=True)\n",
    "        img_path = os.path.join(label_dir, f'image_{i * len(images) + j}.jpg')\n",
    "        torchvision.utils.save_image(image, img_path)\n",
    "\n",
    "print(\"Images saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = InceptionResnetV1(\n",
    "    classify=True,\n",
    "    pretrained='vggface2',\n",
    "    num_classes=len(dataset.class_to_idx)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(resnet.parameters(), lr=0.001)\n",
    "scheduler = MultiStepLR(optimizer, [5, 10])\n",
    "\n",
    "trans = transforms.Compose([\n",
    "    transforms.Resize((160, 160)),\n",
    "    np.float32,\n",
    "    transforms.ToTensor(),\n",
    "    fixed_image_standardization\n",
    "])\n",
    "dataset = datasets.ImageFolder(data_dir + '_cropped', transform=trans)\n",
    "img_inds = np.arange(len(dataset))\n",
    "np.random.shuffle(img_inds)\n",
    "train_inds = img_inds[:int(0.8 * len(img_inds))]\n",
    "val_inds = img_inds[int(0.8 * len(img_inds)):]\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset,\n",
    "    num_workers=workers,\n",
    "    batch_size=batch_size,\n",
    "    sampler=SubsetRandomSampler(train_inds)\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    dataset,\n",
    "    num_workers=workers,\n",
    "    batch_size=batch_size,\n",
    "    sampler=SubsetRandomSampler(val_inds)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "metrics = {\n",
    "    'fps': training.BatchTimer(),\n",
    "    'acc': training.accuracy\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Initial\n",
      "----------\n",
      "Valid |    16/16   | loss:    0.7317 | fps:   59.1714 | acc:    0.4557   \n",
      "\n",
      "Epoch 1/30\n",
      "----------\n",
      "Train |    63/63   | loss:    0.3533 | fps:   26.2659 | acc:    0.8798   \n",
      "Valid |    16/16   | loss:    1.6546 | fps:   87.9705 | acc:    0.7682   \n",
      "\n",
      "Epoch 2/30\n",
      "----------\n",
      "Train |    63/63   | loss:    0.1829 | fps:   27.6840 | acc:    0.9444   \n",
      "Valid |    16/16   | loss:    0.0227 | fps:   81.6568 | acc:    1.0000   \n",
      "\n",
      "Epoch 3/30\n",
      "----------\n",
      "Train |    63/63   | loss:    0.1308 | fps:   29.3208 | acc:    0.9623   \n",
      "Valid |    16/16   | loss:    0.0159 | fps:   91.0847 | acc:    1.0000   \n",
      "\n",
      "Epoch 4/30\n",
      "----------\n",
      "Train |    63/63   | loss:    0.0475 | fps:   29.2221 | acc:    0.9921   \n",
      "Valid |    16/16   | loss:    0.0053 | fps:   87.2514 | acc:    1.0000   \n",
      "\n",
      "Epoch 5/30\n",
      "----------\n",
      "Train |    63/63   | loss:    0.1779 | fps:   27.0312 | acc:    0.9722   \n",
      "Valid |    16/16   | loss:    0.1343 | fps:   89.7069 | acc:    0.9740   \n",
      "\n",
      "Epoch 6/30\n",
      "----------\n",
      "Train |    63/63   | loss:    0.0650 | fps:   28.6627 | acc:    0.9921   \n",
      "Valid |    16/16   | loss:    0.0284 | fps:   86.6100 | acc:    1.0000   \n",
      "\n",
      "Epoch 7/30\n",
      "----------\n",
      "Train |    63/63   | loss:    0.1378 | fps:   28.3572 | acc:    0.9776   \n",
      "Valid |    16/16   | loss:    0.0268 | fps:   87.0974 | acc:    1.0000   \n",
      "\n",
      "Epoch 8/30\n",
      "----------\n",
      "Train |    63/63   | loss:    0.1060 | fps:   29.6738 | acc:    0.9841   \n",
      "Valid |    16/16   | loss:    0.0255 | fps:   87.6154 | acc:    1.0000   \n",
      "\n",
      "Epoch 9/30\n",
      "----------\n",
      "Train |    63/63   | loss:    0.0666 | fps:   27.5993 | acc:    0.9881   \n",
      "Valid |    16/16   | loss:    0.0175 | fps:   79.2693 | acc:    1.0000   \n",
      "\n",
      "Epoch 10/30\n",
      "----------\n",
      "Train |    63/63   | loss:    0.0552 | fps:   29.1301 | acc:    0.9940   \n",
      "Valid |    16/16   | loss:    0.0155 | fps:   84.1791 | acc:    1.0000   \n",
      "\n",
      "Epoch 11/30\n",
      "----------\n",
      "Train |    63/63   | loss:    0.0568 | fps:   29.3248 | acc:    0.9921   \n",
      "Valid |    16/16   | loss:    0.0155 | fps:   90.0163 | acc:    1.0000   \n",
      "\n",
      "Epoch 12/30\n",
      "----------\n",
      "Train |    63/63   | loss:    0.0703 | fps:   30.1344 | acc:    0.9901   \n",
      "Valid |    16/16   | loss:    0.0121 | fps:   92.8244 | acc:    1.0000   \n",
      "\n",
      "Epoch 13/30\n",
      "----------\n",
      "Train |    63/63   | loss:    0.1382 | fps:   29.4791 | acc:    0.9802   \n",
      "Valid |    16/16   | loss:    0.0164 | fps:   74.5308 | acc:    1.0000   \n",
      "\n",
      "Epoch 14/30\n",
      "----------\n",
      "Train |    63/63   | loss:    0.0254 | fps:   26.2036 | acc:    0.9960   \n",
      "Valid |    16/16   | loss:    0.0137 | fps:   85.2577 | acc:    1.0000   \n",
      "\n",
      "Epoch 15/30\n",
      "----------\n",
      "Train |    63/63   | loss:    0.0189 | fps:   29.6118 | acc:    1.0000   \n",
      "Valid |    16/16   | loss:    0.0192 | fps:   89.5366 | acc:    1.0000   \n",
      "\n",
      "Epoch 16/30\n",
      "----------\n",
      "Train |    63/63   | loss:    0.0207 | fps:   28.5065 | acc:    1.0000   \n",
      "Valid |    16/16   | loss:    0.0122 | fps:   90.4650 | acc:    1.0000   \n",
      "\n",
      "Epoch 17/30\n",
      "----------\n",
      "Train |    63/63   | loss:    0.0192 | fps:   29.5464 | acc:    0.9980   \n",
      "Valid |    16/16   | loss:    0.0178 | fps:   87.5043 | acc:    1.0000   \n",
      "\n",
      "Epoch 18/30\n",
      "----------\n",
      "Train |    63/63   | loss:    0.0196 | fps:   30.2750 | acc:    1.0000   \n",
      "Valid |    16/16   | loss:    0.0107 | fps:   90.8962 | acc:    1.0000   \n",
      "\n",
      "Epoch 19/30\n",
      "----------\n",
      "Train |    63/63   | loss:    0.0229 | fps:   30.0688 | acc:    0.9960   \n",
      "Valid |    16/16   | loss:    0.0087 | fps:   89.2514 | acc:    1.0000   \n",
      "\n",
      "Epoch 20/30\n",
      "----------\n",
      "Train |    63/63   | loss:    0.0159 | fps:   29.8719 | acc:    0.9980   \n",
      "Valid |    16/16   | loss:    0.0105 | fps:   90.3426 | acc:    1.0000   \n",
      "\n",
      "Epoch 21/30\n",
      "----------\n",
      "Train |    63/63   | loss:    0.0180 | fps:   28.5018 | acc:    0.9980   \n",
      "Valid |    16/16   | loss:    0.0115 | fps:   74.1337 | acc:    1.0000   \n",
      "\n",
      "Epoch 22/30\n",
      "----------\n",
      "Train |    63/63   | loss:    0.0139 | fps:   28.1993 | acc:    1.0000   \n",
      "Valid |    16/16   | loss:    0.0095 | fps:   88.8953 | acc:    1.0000   \n",
      "\n",
      "Epoch 23/30\n",
      "----------\n",
      "Train |    63/63   | loss:    0.0125 | fps:   29.0678 | acc:    1.0000   \n",
      "Valid |    16/16   | loss:    0.0094 | fps:   85.2787 | acc:    1.0000   \n",
      "\n",
      "Epoch 24/30\n",
      "----------\n",
      "Train |    63/63   | loss:    0.0167 | fps:   29.7412 | acc:    1.0000   \n",
      "Valid |    16/16   | loss:    0.0098 | fps:   85.8357 | acc:    1.0000   \n",
      "\n",
      "Epoch 25/30\n",
      "----------\n",
      "Train |    63/63   | loss:    0.0152 | fps:   30.1733 | acc:    1.0000   \n",
      "Valid |    16/16   | loss:    0.0074 | fps:   89.1149 | acc:    1.0000   \n",
      "\n",
      "Epoch 26/30\n",
      "----------\n",
      "Train |    63/63   | loss:    0.0160 | fps:   30.0684 | acc:    1.0000   \n",
      "Valid |    16/16   | loss:    0.0077 | fps:   88.4979 | acc:    1.0000   \n",
      "\n",
      "Epoch 27/30\n",
      "----------\n",
      "Train |    63/63   | loss:    0.0067 | fps:   29.8167 | acc:    1.0000   \n",
      "Valid |    16/16   | loss:    0.0070 | fps:   88.4044 | acc:    1.0000   \n",
      "\n",
      "Epoch 28/30\n",
      "----------\n",
      "Train |    63/63   | loss:    0.0633 | fps:   28.7339 | acc:    0.9940   \n",
      "Valid |    16/16   | loss:    0.0075 | fps:   84.7588 | acc:    1.0000   \n",
      "\n",
      "Epoch 29/30\n",
      "----------\n",
      "Train |    63/63   | loss:    0.0139 | fps:   29.3203 | acc:    0.9980   \n",
      "Valid |    16/16   | loss:    0.0087 | fps:   87.8199 | acc:    1.0000   \n",
      "\n",
      "Epoch 30/30\n",
      "----------\n",
      "Train |    63/63   | loss:    0.0121 | fps:   28.6520 | acc:    1.0000   \n",
      "Valid |    16/16   | loss:    0.0058 | fps:   82.7794 | acc:    1.0000   \n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter()\n",
    "writer.iteration, writer.interval = 0, 10\n",
    "\n",
    "print('\\n\\nInitial')\n",
    "print('-' * 10)\n",
    "resnet.eval()\n",
    "training.pass_epoch(\n",
    "    resnet, loss_fn, val_loader,\n",
    "    batch_metrics=metrics, show_running=True, device=device,\n",
    "    writer=writer\n",
    ")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('\\nEpoch {}/{}'.format(epoch + 1, epochs))\n",
    "    print('-' * 10)\n",
    "\n",
    "    resnet.train()\n",
    "    training.pass_epoch(\n",
    "        resnet, loss_fn, train_loader, optimizer, scheduler,\n",
    "        batch_metrics=metrics, show_running=True, device=device,\n",
    "        writer=writer\n",
    "    )\n",
    "\n",
    "    resnet.eval()\n",
    "    training.pass_epoch(\n",
    "        resnet, loss_fn, val_loader,\n",
    "        batch_metrics=metrics, show_running=True, device=device,\n",
    "        writer=writer\n",
    "    )\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "6aNOYxGm9qh4"
   },
   "outputs": [],
   "source": [
    "mtcnn0=MTCNN(image_size=240,margin=0,keep_all=False,min_face_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "LgFzELMOFS1m"
   },
   "outputs": [],
   "source": [
    "mtcnn=MTCNN(image_size=240,margin=0,keep_all=True,min_face_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "2MiUrUMSFeMS"
   },
   "outputs": [],
   "source": [
    "dataset=datasets.ImageFolder('Faces')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "m2oKJPkRGQug"
   },
   "outputs": [],
   "source": [
    "idx_to_class={i:c for c,i in dataset.class_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "LA2ZsTe7JNH3"
   },
   "outputs": [],
   "source": [
    "def collate_fn(x):\n",
    "  return x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "omyyTRS8Hn6n"
   },
   "outputs": [],
   "source": [
    "loader=DataLoader(dataset,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1vRJuzfzIapN"
   },
   "outputs": [],
   "source": [
    "name_list=[]\n",
    "embedding_list=[]\n",
    "\n",
    "resnet.to(device)\n",
    "\n",
    "for img,idx in loader:\n",
    "  # print(img)\n",
    "  # print(idx)\n",
    "  face,prob=mtcnn0(img,return_prob=True)\n",
    "  if face is not None and prob>0.9:\n",
    "    face = face.to(device)\n",
    "    emb=resnet((face.unsqueeze(0)))\n",
    "    embedding_list.append(emb)\n",
    "    name_list.append(idx_to_class[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving embedding_list and name_list\n",
    "with open('NamitKush30_emb/embedding_list.pkl', 'wb') as f:\n",
    "    pickle.dump(embedding_list, f)\n",
    "\n",
    "with open('NamitKush30_emb/name_list.pkl', 'wb') as f:\n",
    "    pickle.dump(name_list, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading embedding_list and name_list\n",
    "with open('NamitKush30_emb/embedding_list.pkl', 'rb') as f:\n",
    "    embedding_list = pickle.load(f)\n",
    "\n",
    "with open('NamitKush30_emb/name_list.pkl', 'rb') as f:\n",
    "    name_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frames per second = 31\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_36340\\691849214.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  face_tensor = torch.tensor(img_cropped_list[i], dtype=torch.float32).unsqueeze(0).to(device).detach()\n"
     ]
    }
   ],
   "source": [
    "# cv2.namedWindow(\"preview\")\n",
    "# # video = cv2.VideoCapture('Namit.mp4')\n",
    "# # video = cv2.VideoCapture('20240418_122036.mp4')\n",
    "# video = cv2.VideoCapture('video2.mp4')\n",
    "# fps = math.ceil(video.get(cv2.CAP_PROP_FPS))\n",
    "# print('frames per second =',fps)\n",
    "\n",
    "# if not video.isOpened():\n",
    "#     print(\"Error: Could not open video.\")\n",
    "\n",
    "# interval_frames = int(fps * 1/30)\n",
    "# print(interval_frames)\n",
    "\n",
    "# target_size = (1536, 864)\n",
    "\n",
    "# fc=0\n",
    "# resnet.to(device)\n",
    "# while True:\n",
    "#   ret,frame=video.read()\n",
    "# #   print(frame.shape)\n",
    "# #   break\n",
    "#   if not ret:\n",
    "#     break\n",
    "  \n",
    "#   resized_frame = cv2.resize(frame, target_size)\n",
    "  \n",
    "#   if fc%interval_frames==0:\n",
    "#       img=Image.fromarray(resized_frame)\n",
    "#       img_cropped_list,prob_list=mtcnn(img,return_prob=True)\n",
    "#       if img_cropped_list is not None:\n",
    "#          boxes,_=mtcnn.detect(img)\n",
    "#          for i,prob in enumerate(prob_list):\n",
    "#              if prob>0.9:\n",
    "#                  face_tensor = torch.tensor(img_cropped_list[i], dtype=torch.float32).unsqueeze(0).to(device).detach()\n",
    "#                  emb = resnet(face_tensor).detach()\n",
    "#                 #  emb=resnet(img_cropped_list[i].unsqueeze(0)).detach()\n",
    "#                  dist_list=[]\n",
    "#                  for idx,emb_db in enumerate(embedding_list):\n",
    "#                      dist=torch.dist(emb,emb_db).item()\n",
    "#                      dist_list.append(dist)\n",
    "#                  min_dist=min(dist_list)\n",
    "#                  box=boxes[i]\n",
    "#                  if min_dist<0.2:\n",
    "#                      min_dist_idx=dist_list.index(min_dist)\n",
    "#                      name=name_list[min_dist_idx]\n",
    "#                 #  original_frame=resized_frame.copy()\n",
    "#                      resized_frame = cv2.putText(resized_frame, name+' '+str(min_dist), (int(box[0]),int(box[1])), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0),1, cv2.LINE_AA)\n",
    "#                  resized_frame = cv2.rectangle(resized_frame, (int(box[0]),int(box[1])) , (int(box[2]),int(box[3])), (255,0,0), 2)\n",
    "#       cv2.imshow(\"preview\",resized_frame)\n",
    "#       if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "#           break\n",
    "#   fc+= 1\n",
    "# video.release()\n",
    "# cv2.destroyAllWindows()\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=2\n",
    "v=[]\n",
    "count=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(n):\n",
    "    # print(j)\n",
    "    video = cv2.VideoCapture(f'videos2/video{j+1}.mp4')\n",
    "    fps = math.ceil(video.get(cv2.CAP_PROP_FPS))\n",
    "    print('frames per second =',fps)\n",
    "    if not video.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "    interval_frames = int(fps * 1/30)\n",
    "    print('interval_frames=',interval_frames)\n",
    "    fc=0\n",
    "    while (video.isOpened()):\n",
    "      ret,frame=video.read()\n",
    "      if not ret:\n",
    "        break\n",
    "      if fc%interval_frames==0:\n",
    "          img=Image.fromarray(frame)\n",
    "          img_cropped_list,prob_list=mtcnn(img,return_prob=True)\n",
    "          if img_cropped_list is not None:\n",
    "             boxes,_=mtcnn.detect(img)\n",
    "             for i,prob in enumerate(prob_list):\n",
    "                 if prob>0.9:\n",
    "                     face_tensor = torch.tensor(img_cropped_list[i], dtype=torch.float32).unsqueeze(0).to(device).detach()\n",
    "                     emb = resnet(face_tensor).detach()\n",
    "                     dist_list=[]\n",
    "                     for idx,emb_db in enumerate(embedding_list):\n",
    "                         dist=torch.dist(emb,emb_db).item()\n",
    "                         dist_list.append(dist)\n",
    "                     min_dist=min(dist_list)\n",
    "                     min_dist_idx=dist_list.index(min_dist)\n",
    "                     name=name_list[min_dist_idx]\n",
    "                     box=boxes[i]\n",
    "                     original_frame=frame.copy()\n",
    "                     if min_dist<0.5:\n",
    "                         frame = cv2.putText(frame, name+' '+str(min_dist), (int(box[0]),int(box[1])), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0),1, cv2.LINE_AA)\n",
    "                     frame = cv2.rectangle(frame, (int(box[0]),int(box[1])) , (int(box[2]),int(box[3])), (255,0,0), 2)\n",
    "                     ts = video.get(cv2.CAP_PROP_POS_MSEC)\n",
    "                     resized_frame = cv2.resize(frame, (3840,2160))\n",
    "                     print(\"j=\",j+1)\n",
    "                     v.append((resized_frame,ts))\n",
    "                     count+=1\n",
    "          # cv2.imshow(\"preview\",frame)\n",
    "          if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "              break\n",
    "      fc+= 1\n",
    "    video.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv= sorted(v, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sv:\n",
    "    v.append((i[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_video_from_frames(frames, output_video_path, frame_rate):\n",
    "    frame_shape = frames[0].shape\n",
    "    height, width, _ = frame_shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, frame_rate, (width, height))\n",
    "    for frame in frames:\n",
    "        out.write(frame)\n",
    "\n",
    "    out.release()\n",
    "\n",
    "\n",
    "output_video_path = \"output2_video.mp4\"\n",
    "frame_rate = 60\n",
    "create_video_from_frames(v, output_video_path, frame_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
